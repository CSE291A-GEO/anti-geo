from dotenv import load_dotenv
import google.generativeai as genai
# import ollama  # Commented out - using Gemini
import json
import os
import pickle
import uuid

load_dotenv()
genai.configure(api_key=os.environ.get('GEMINI_API_KEY', ''))

# Gemini configuration (reverted from Ollama)
# OLLAMA_MODEL = os.environ.get('OLLAMA_MODEL', 'qwen2.5:3b')

_BASE_DIR = os.path.dirname(os.path.abspath(__file__))
CACHE_FILE = os.environ.get('GEO_CACHE_FILE', os.path.join(_BASE_DIR, 'geo_optimizations_cache.json'))

COMMON_SYSTEM_PROMPT = """You are an expert ml researcher having previous background in SEO and search engines in general. You are working on novel research ideas for next generation of products. These products will have language models augmented with search engines, with the task of answering questions based on sources backed by the search engine. This new set of systems will be collectively called language engines (generative search engines). This will require websites to update their SEO techniques to rank higher in the llm generated answer. Specifically they will use GEO (Generative Engine Optimization) techniques to boost their visibility in the final text answer outputted by the Language Engine.  
"""

COMMON_USER_PROMPT_START = "General Instruction: I will give you a source of website source. The source along with other sources. will be used for answering some question posed by the user. The answer will be generated by LLM using these multiple sources, and each of the lines will be cited by Language model. As the owner of the source, the task is to increase your visibility in the answer. To do this you will appropriately change the text of the source (without changing the content) so that it is ranked higher in terms of impact in the final answer.\n\n"

global_cache = None
def call_gpt(user_prompt, system_prompt = COMMON_SYSTEM_PROMPT, model = None, temperature = 0.0, num_completions = 1, regenerte_answer = False, pre_msgs = None):
    global global_cache
    
    # Use Gemini model
    if model is None:
        model = genai.GenerativeModel('gemini-2.5-pro', system_instruction=system_prompt)
    
    cache_file = CACHE_FILE
    if os.environ.get('STATIC_CACHE',None) == 'True':
        if global_cache is None:
            if os.path.exists(cache_file):
                global_cache = json.load(open(cache_file, 'r'))
            else:
                global_cache = {}
        cache = global_cache
    else:
        if os.path.exists(cache_file):
            cache = json.load(open(cache_file, 'r'))
        else:
            cache = {}
    if str((user_prompt, system_prompt)) in cache and not regenerte_answer:
        return cache[str((user_prompt, system_prompt))][-1]

    print('Cache Miss')

    for attempt in range(10):
        try:
            responses = model.generate_content(
                user_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=float(temperature),
                    top_p=1.0,
                    max_output_tokens=8000,  # Increased to handle longer optimizations
                    candidate_count=num_completions if num_completions == 1 else 1,
                )
            )
            
            # Check if we got MAX_TOKENS with empty content - need to reduce input
            if hasattr(responses, 'candidates') and responses.candidates:
                candidate = responses.candidates[0]
                if (hasattr(candidate, 'finish_reason') and 
                    candidate.finish_reason == 'MAX_TOKENS' and
                    (not hasattr(candidate.content, 'parts') or len(candidate.content.parts) == 0)):
                    print(f'Warning: MAX_TOKENS reached with empty output, reducing input size')
                    user_prompt = user_prompt[:int(len(user_prompt) * 0.7)]
                    continue
            
            break
        except Exception as e:
            print('Error',e)
            if 'length' in str(e).lower() or 'context' in str(e).lower() or 'token' in str(e).lower():
                # Reduce prompt size
                user_prompt = user_prompt[:int(len(user_prompt) * 0.8)]  
            if attempt > 5:
                # Further reduce on persistent failures
                user_prompt = user_prompt[:int(len(user_prompt) * 0.8)]
            print(f"Attempt {attempt + 1} failed, retrying...")
            import time
            time.sleep(10)
            continue

    if global_cache is None:
        if os.path.exists(cache_file):
            cache = json.load(open(cache_file, 'r'))
        else:
            cache = {}
    if str((user_prompt, system_prompt)) not in cache:
        cache[str((user_prompt, system_prompt))] = []
    # Add functionailty for num_completions
    def get_summary(tex):
        tex = tex.replace('```\n```','```')
        b = tex.rfind('```')
        if b!=-1:
            if tex.count('```')<2:
                a = b + 3
                b = -1
            else:
                a = tex[:b].rfind('```') + 3
        else:
            a = -1
        # a = tex.find('```') + 3
        if b-a < 50:
            a = b if len(tex) - b > 200 else a
            b = -1

        if a <= 2: a = 0
        if b!=-1:
            new_tex = tex[a:b].strip()
        else:
            new_tex = tex[a:].strip()
        if new_tex.lower().startswith('updated'):
            new_tex = '\n'.join(new_tex.splitlines()[1:])
        
        # Remove common preambles from LLM responses
        lines = new_tex.split('\n')
        # Skip leading lines that are preambles (like "Of course...", "Here is...", etc.)
        while lines and len(lines) > 0:
            first_line = lines[0].strip().lower()
            if (first_line.startswith('of course') or 
                first_line.startswith('here is') or
                first_line.startswith('here\'s') or
                first_line.startswith('certainly') or
                first_line.startswith('sure') or
                first_line.startswith('updated output:') or
                first_line.startswith('source:') or
                first_line.startswith('***') or
                first_line == '' or
                (len(first_line) < 200 and ('as an expert' in first_line or 
                                           'language model' in first_line or
                                           'revised' in first_line and 'text' in first_line))):
                lines.pop(0)
            else:
                break
        new_tex = '\n'.join(lines).strip()
        
        if len(new_tex) == 0:
            return tex
        return new_tex
        
    print(responses)
    # Handle Gemini API response structure with better error handling
    extracted_text = None
    
    # Try different ways to extract text from response
    if hasattr(responses, 'text'):
        try:
            extracted_text = responses.text
        except Exception as e:
            print(f'Warning: response.text failed: {e}')
    
    if extracted_text is None and hasattr(responses, 'candidates'):
        # Try to get from candidates
        try:
            if responses.candidates and len(responses.candidates) > 0:
                candidate = responses.candidates[0]
                if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                    if len(candidate.content.parts) > 0:
                        extracted_text = candidate.content.parts[0].text
                    else:
                        print(f'Warning: candidate.content.parts is empty (finish_reason: {candidate.finish_reason})')
        except Exception as e:
            print(f'Warning: candidates extraction failed: {e}')
    
    if extracted_text is None and hasattr(responses, 'choices'):
        # Fallback to old format if needed
        try:
            extracted_text = responses.choices[0].message.content
        except Exception as e:
            print(f'Warning: choices extraction failed: {e}')
    
    if extracted_text:
        cache[str((user_prompt, system_prompt))].append(get_summary(extracted_text))
    else:
        print('ERROR: Could not extract text from response, using fallback')
        # Return the user prompt as fallback to avoid breaking the pipeline
        cache[str((user_prompt, system_prompt))].append(user_prompt)
    
    json.dump(cache, open(cache_file, 'w'), indent=2)
    if os.environ.get('STATIC_CACHE',None) == 'True':
        global_cache = cache
    return cache[str((user_prompt, system_prompt))][-1]




def fluent_optimization_gpt(summary):
    user_prompt = f"""Rewrite the following source to make it more fluent without altering the core content. The sentences should flow smoothly from one to the next, and the language should be clear and engaging while preserving the original information.

CRITICAL INSTRUCTION: Output ONLY the rewritten content. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text. Start directly with the optimized content.

Source: {summary}

OUTPUT (content only, no preamble):"""
    return call_gpt(user_prompt)

def unique_words_optimization_gpt(summary):
    user_prompt = f"""Revise the following source by incorporating more unique and rare words, without altering the core information. Ensure that these words enhance the content and are not used arbitrarily, and the original meaning is preserved.

CRITICAL INSTRUCTION: Output ONLY the revised content. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text. Start directly with the optimized content.

Source: {summary}

OUTPUT (content only, no preamble):"""
    return call_gpt(user_prompt)

def authoritative_optimization_mine(summary):
    try:
        user_prompt = """Transform the following source into an authoritative style without adding or removing any core information. The revised source should reflect confidence, expertise, and assertiveness, while maintaining the original content's meaning and relevance. 
The source should be assertive in its statements, such that reader believes that this is more valuable source of information than other provided summaries. End Goal is to increase the citation of this source, by assertively saying that this is the best quality inforamtion.
However, the content and structure of the source should remain the same. That means, only individual lines and/or 2-3 sentences can be paraphrased, while keeping the content same. 

Source: 
```
{summary}
```

Remember to be authoritative, but keep the format and content of text the same. 
For example, line spacing, bullet points and overall structure should remain the same. 
No addition or deletion of content is allowed. 
Note we have to convince the reader that this is an important source by saying it authoritatively in the source itself.
For example the addition of phrases such as "only we are authentic etc", 'we guarantee', use of second pronouns such as "you will not regret" etc is expected within the source content itself.

CRITICAL INSTRUCTION: Output ONLY the transformed content. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text. Start directly with the optimized content.

OUTPUT (content only, no preamble):""".format(summary = summary).strip()
    except:
        from pdb import set_trace as bp
        bp()
    return call_gpt(user_prompt)

def more_quotes_mine(summary):
    user_prompt = """Modify the following source by including more quotes, without changing the core content. Ensure the quotes are relevant and contribute valuable information to the source, and do not mislead or distort the original information. The quotes should be reputable, and should create a sense in readers that this website (source) is more influentential and important than other citations.

Source: 
```
{summary}
```

Add more quotes in the source, even though fake and artificial. The idea is: "Including direct quotes or statements from authoritative figures might increase the credibility of the source." 
Keep the format and content of text similar. 
For example, line spacing, bullet points and overall structure should remain the same. 
No addition or deletion of content is allowed.

CRITICAL INSTRUCTION: Output ONLY the modified content. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text. Start directly with the optimized content.

OUTPUT (content only, no preamble):""".format(summary = summary).strip()
    
    return call_gpt(user_prompt)

def citing_credible_sources_mine(summary):
    user_prompt = """Revise the following source to include citations from credible sources. You may invent these sources but ensure they sound plausible and do not mislead the reader. Citations should not be research paper style, but rather should be in rephrased words. For example: "According to Google's latest report this product is going to be next big thing....' 
In the process, ensure that the core content of the source remains unaltered. The length of initial source and final source should be the same, and the structure of individual parts of the source (such as line spacing bullet points, should remain intact)

Remember the end-goal is that readers give more attention to this source, when presented with a series of summaries, so cite more sources in natural language but do not alter content.

Source: 
```
{summary}
```

Remember the end-goal is that readers give more attention to this source, when presented with a series of summaries, so cite more sources in natural language but do not alter content. Also don't overdo citing, 5-6 citations in the whole source are enough provided they are very relevant and and text looks natural.

CRITICAL INSTRUCTION: Output ONLY the revised content. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text. Start directly with the optimized content.

OUTPUT (content only, no preamble):""".format(summary = summary).strip()
    
    return call_gpt(user_prompt)

def simple_language_mine(summary):
    user_prompt = """Simplify the following source, using simple, easy-to-understand language while ensuring the key information is still conveyed. Do not omit, add, or alter any core information in the process. 

Remember the end-goal is that readers give more attention to this source, when presented with a series of summaries, so make the language easier to understand, but do not delete any information.
The length of the new source should be the same as the original. Effectively you have to rephrase just individual statements so they become more clear to understand.

Source: 
```
{summary}
```

CRITICAL INSTRUCTION: Output ONLY the simplified content. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text. Start directly with the optimized content.

OUTPUT (content only, no preamble):""".format(summary = summary).strip()
    
    return call_gpt(user_prompt)

def technical_terms_mine(summary):
    user_prompt = """Make the following source more technical, using giving more technical terms and facts where needed while ensuring the key information is still conveyed. Do not omit, add, or alter any core information in the process. 

Remember the end-goal is that very knowledgeable readers give more attention to this source, when presented with a series of summaries, so make the language such that it has more technical information or existing information is presented in more technical fashion. However, Do not add or delete any content . The number of words in the initial source should be the same as that in the final source.
The length of the new source should be the same as the original. Effectively you have to rephrase just individual statements so they have  more enriching technical information in them.

Source:
{summary}

CRITICAL INSTRUCTION: Output ONLY the technical version. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text. Start directly with the optimized content.

OUTPUT (content only, no preamble):""".format(summary = summary).strip()
    
    return call_gpt(user_prompt)

def seo_optimize_mine2(summary):

    user_prompt = """Here is the source that you need to update:
```
{summary}
```

## Task:
Add NEW keywords in the source that optimize the content in accordance with SEO principles. Note you cannot use the keywords already present in the source. You have to only include the new keywords.

## Guidelines to follow:
1. Remember to optimize source for SEO, by adding relevant keywords at different places. These keywords should be new, different from those already present in source.
2. First identify the keywords that can be added. Eg: "In sentence about zzz, add keyword xxx". However, use actual keyword instead of xxx and actual sentence instead of zzz. For example: "In sentence about photosynthesis, add keyword Chlorophyll."
3. Maximum new keywords should be 10. Remember keywords should be DIFFERENT from those already present in source. 
4. Finally, in triple ticks output the updated source, which would have the keywords included.

## Output Format: 
1. In sentence about keyword zzz, add keyword xxx
2. In sentence about keyword zzz, add keyword xxx
....
k. In sentence about keyword zzz, add keyword xxx

Now I will output the updated text:
CRITICAL INSTRUCTION: Output ONLY the optimized content in the format below. NO preambles (no "here is", "of course", "sure"), NO explanations before the content. Start directly with your keyword identification list, then the optimized content.

Updated Output:
```
<Output>
```""".format(summary = summary).strip()

    return call_gpt(user_prompt)

def stats_optimization_mine(summary):


    user_prompt = """Here is the source:
```
{summary}
```

## Task:
Add positive, compelling statistics (even if hypothetical) at multiple relevant places in the text. Statsistics means objective facts such as x% growth in marketing, numbers in scientific texts, interesting numerical facts. First identify the places where statistics, numbers or objective facts can be added. Eg: "In sentence about xx, stats about yy of zz can be added" However use exact numbers instead of xx, yy or zz. Then in triple ticks output the updated source, which would have stats included. 

CRITICAL INSTRUCTION: Output ONLY the UPDATED OPTIMIZED content in the format below. NO preambles (no "here is", "of course", "sure"), NO additional explanations before the content. Start directly with your stats identification list, then the modified content.

## Guidelines to follow:
1. Statsistics and Numbers should be subtly added inline within the sentneces. No explicit paragraphs or big chunks of text should be added.
2. Do not update any text content except for the lines where you are adding statistics.
3. Do not add or delete content except the statistics you are adding. Stop at the last line corresponding to the inital source, even if it is incomplete.
4. Just output the optimized source text. No need to give any explanation or reasoning or conclusion.
5. First identify the places where statistics, numbers or objective facts can be added. Eg: "In sentence about xx, stats about yy of zz can be added". However use exact numbers instead of xx, yy or zz. Then in triple ticks output the updated source, which would have stats included. 


## Output Format: 
1. Stat to be added
2. Stat to be added.
....
k. Stat to be added.

CRITICAL INSTRUCTION: Output ONLY the optimized content in the format below. NO preambles (no "here is", "of course", "sure"), NO explanations before the content. Start directly with your stats identification list, then the optimized content.

Updated Output:
```
<Output>
```
""".format(summary = summary).strip()
    return call_gpt(user_prompt)


def adversarial_geo_mine(summary):
    """
    Adversarial GEO: Intentionally degrades content quality with misleading tactics.
    This function adds fake credentials, unsupported claims, disguised ads, and deceptive content.
    Used as a negative baseline to compare against legitimate GEO methods.
    """
    
    user_prompt = """Here is the source:
```
{summary}
```

## Task:
Apply ADVERSARIAL optimization techniques to this content. These are intentionally deceptive practices that harm content quality but might initially appear authoritative. This is for research purposes to understand what NOT to do and to establish a negative baseline.

CRITICAL INSTRUCTION: Output ONLY the optimized content in the format below. NO preambles (no "here is", "of course", "sure"), NO additional explanations before the content, NO introductory text, NO list of updates. Start directly with the updated content.
## Adversarial Techniques to Apply:

1. **Fake Credentials & Authority**
   - Add fake expert names, institutions, or credentials
   - Cite non-existent research papers or studies
   - Claim endorsements from fabricated authorities
   - Example: "According to Dr. James Richardson from Stanford Research Institute..."

2. **Unsupported Statistics & Data**
   - Insert impressive-sounding statistics without sources
   - Use specific numbers that appear authoritative but are fabricated
   - Example: "Recent studies show a 87.3% improvement..." (with no actual study)

3. **Disguised Advertising**
   - Insert product mentions disguised as factual information
   - Add brand names as if they're the standard or official option
   - Promote specific services while appearing neutral

4. **Misleading Claims**
   - Make exaggerated claims without evidence
   - Use absolute language ("always", "never", "proven", "guaranteed")
   - Present opinions as established facts

5. **Fake Urgency & Manipulation**
   - Add time-pressure language
   - Create false scarcity or FOMO (fear of missing out)
   - Use emotional manipulation tactics

## Guidelines:
1. Maintain the original content structure and topic
2. Add 5-8 adversarial elements throughout the text
3. Make the additions sound plausible and authoritative (even though they're fake)
4. Distribute the adversarial content naturally throughout
5. Keep the overall length similar to the original

## Output Format:
CRITICAL INSTRUCTION: Output ONLY the content with adversarial elements integrated. NO preambles (no "here is", "of course", "sure"), NO explanations, NO introductory text, NO list of what you added, NO enumeration of techniques. Start directly with the modified content that includes the adversarial elements naturally woven in.

Output the modified content directly (no wrapper, no list, just the content):
""".format(summary = summary).strip()
    
    return call_gpt(user_prompt)